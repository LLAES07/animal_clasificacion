# ü§ñ **Clasificador de Animales con Deep Learning: De 61% a 97% de Precisi√≥n**

Este repositorio documenta el viaje completo y metodol√≥gico para construir un clasificador de im√°genes de alto rendimiento. Partiendo de un modelo b√°sico con problemas de sobreajuste, el proyecto avanza a trav√©s de iteraciones de mejora, culminando en un modelo de vanguardia que utiliza Transfer Learning y Fine-Tuning para alcanzar una precisi√≥n del 97.39% en la clasificaci√≥n de 10 tipos de animales.


# üöÄ **Resumen del Proyecto**

El objetivo es simple: clasificar im√°genes de animales. Sin embargo, el camino para lograrlo implic√≥ un proceso iterativo de diagn√≥stico, hip√≥tesis y experimentaci√≥n.

Este proyecto demuestra:
- La construcci√≥n de **Redes Neuronales Convolucionales (CNNs)** desde cero.

- El diagn√≥stico y la soluci√≥n de problemas comunes como el **sobreajuste (overfitting)** y la inestabilidad del entrenamiento.

- La aplicaci√≥n de un arsenal de **t√©cnicas de regularizaci√≥n** (Dropout, BatchNormalization, L2, GlobalAveragePooling2D).

- La implementaci√≥n experta de **Transfer Learning** con **MobileNetV2**.

- La optimizaci√≥n final usando **Fine-Tuning** para exprimir el m√°ximo rendimiento.


## 1Ô∏è‚É£ **Modelo 1: Un Comienzo Humilde (Precisi√≥n ~54%)**

El primer intento fue una CNN secuencial est√°ndar. Aunque aprendi√≥ patrones b√°sicos, los resultados revelaron problemas cr√≠ticos.

- **Diagn√≥stico**: Un severo sobreajuste (gran brecha entre la precisi√≥n de entrenamiento y la de validaci√≥n) y una alta inestabilidad (curvas de validaci√≥n ruidosas y con picos).

- **Aprendizaje Clave**: Un modelo sin una fuerte regularizaci√≥n tiende a memorizar los datos de entrenamiento en lugar de generalizar.

![grafica modelo 1](image.png)

## 2Ô∏è‚É£ **Modelo 2: Domesticando el Overfitting (Precisi√≥n ~86%)**
Armado con el diagn√≥stico anterior, este modelo fue dise√±ado para ser m√°s robusto y generalizar mejor.

**Mejoras Estrat√©gicas:** 
    - GlobalAveragePooling2D para reemplazar la capa Flatten, reduciendo dr√°sticamente los par√°metros.

    - Regularizaci√≥n L2 en las capas convolucionales para penalizar la complejidad del modelo.

    - BatchNormalization para estabilizar y acelerar el entrenamiento.

    - Arquitectura de la red m√°s profunda para que pueda aprender mayores caracteristicas.

**Resultado**: Un salto masivo en rendimiento y estabilidad. El sobreajuste se control√≥ significativamente.

![grafica modelo 2](image-1.png)


## 3Ô∏è‚É£ **Modelo 3: Alcanzando la Excelencia con Transfer Learning (Precisi√≥n Final 96.29%)**
Para superar el techo del 86%, introduje la idea de utilizar: Transfer Learning.

- **Fase 1:** Extractor de Caracter√≠sticas (Precisi√≥n ~94%)
Se utiliz√≥ MobileNetV2 pre-entrenado en ImageNet con sus capas congeladas.
Solo se entren√≥ una nueva "cabeza" clasificadora. El resultado fue una convergencia explosiva y un rendimiento excelente.

- **Fase 2:** Fine-Tuning (Precisi√≥n Final 96.29%)

Se "descongelaron" las capas superiores de MobileNetV2.
Se continu√≥ el entrenamiento con una tasa de aprendizaje de (1e-5) para ajustar sutilmente el modelo a nuestro dataset espec√≠fico.
Esta fase final puli√≥ el modelo, mejorando el rendimiento en todas las clases y alcanzando un estado de vanguardia.

![alt text](image-2.png)

![alt text](image-3.png)


# üìä **Resultados Finales**
El modelo final, despu√©s del fine-tuning, fue evaluado en un conjunto de prueba independiente, con los siguientes resultados:

**Precisi√≥n Global: 96.29%**

![alt text](image-4.png)


# **üõ†Ô∏è Stack Tecnol√≥gico**

**Lenguaje:** Python 3
**Librer√≠as Principales:** TensorFlow, Keras, Scikit-learn, NumPy, Pandas, Matplotlib
#  **Clasificador de Animales con Deep Learning: De 61% a 97% de Precisi贸n**

Este repositorio documenta el viaje completo y metodol贸gico para construir un clasificador de im谩genes de alto rendimiento. Partiendo de un modelo b谩sico con problemas de sobreajuste, el proyecto avanza a trav茅s de iteraciones de mejora, culminando en un modelo de vanguardia que utiliza Transfer Learning y Fine-Tuning para alcanzar una precisi贸n del 97.39% en la clasificaci贸n de 10 tipos de animales.


#  **Resumen del Proyecto**

El objetivo es simple: clasificar im谩genes de animales. Sin embargo, el camino para lograrlo implic贸 un proceso iterativo de diagn贸stico, hip贸tesis y experimentaci贸n.

Este proyecto demuestra:
- La construcci贸n de **Redes Neuronales Convolucionales (CNNs)** desde cero.

- El diagn贸stico y la soluci贸n de problemas comunes como el **sobreajuste (overfitting)** y la inestabilidad del entrenamiento.

- La aplicaci贸n de un arsenal de **t茅cnicas de regularizaci贸n** (Dropout, BatchNormalization, L2, GlobalAveragePooling2D).

- La implementaci贸n experta de **Transfer Learning** con **MobileNetV2**.

- La optimizaci贸n final usando **Fine-Tuning** para exprimir el m谩ximo rendimiento.


## 1锔 **Modelo 1: Un Comienzo Humilde (Precisi贸n ~54%)**

El primer intento fue una CNN secuencial est谩ndar. Aunque aprendi贸 patrones b谩sicos, los resultados revelaron problemas cr铆ticos.

- **Diagn贸stico**: Un severo sobreajuste (gran brecha entre la precisi贸n de entrenamiento y la de validaci贸n) y una alta inestabilidad (curvas de validaci贸n ruidosas y con picos).

- **Aprendizaje Clave**: Un modelo sin una fuerte regularizaci贸n tiende a memorizar los datos de entrenamiento en lugar de generalizar.

![grafica modelo 1](image.png)

## 2锔 **Modelo 2: Domesticando el Overfitting (Precisi贸n ~86%)**
Armado con el diagn贸stico anterior, este modelo fue dise帽ado para ser m谩s robusto y generalizar mejor.

**Mejoras Estrat茅gicas:** 
    - GlobalAveragePooling2D para reemplazar la capa Flatten, reduciendo dr谩sticamente los par谩metros.

    - Regularizaci贸n L2 en las capas convolucionales para penalizar la complejidad del modelo.

    - BatchNormalization para estabilizar y acelerar el entrenamiento.

    - Arquitectura de la red m谩s profunda para que pueda aprender mayores caracteristicas.

**Resultado**: Un salto masivo en rendimiento y estabilidad. El sobreajuste se control贸 significativamente.

![grafica modelo 2](image-1.png)


## 3锔 **Modelo 3: Alcanzando la Excelencia con Transfer Learning (Precisi贸n Final 96.29%)**
Para superar el techo del 86%, introduje la idea de utilizar: Transfer Learning.

- **Fase 1:** Extractor de Caracter铆sticas (Precisi贸n ~94%)
Se utiliz贸 MobileNetV2 pre-entrenado en ImageNet con sus capas congeladas.
Solo se entren贸 una nueva "cabeza" clasificadora. El resultado fue una convergencia explosiva y un rendimiento excelente.

- **Fase 2:** Fine-Tuning (Precisi贸n Final 96.29%)

Se "descongelaron" las capas superiores de MobileNetV2.
Se continu贸 el entrenamiento con una tasa de aprendizaje de (1e-5) para ajustar sutilmente el modelo a nuestro dataset espec铆fico.
Esta fase final puli贸 el modelo, mejorando el rendimiento en todas las clases y alcanzando un estado de vanguardia.

![alt text](image-2.png)

![alt text](image-3.png)